/**
 * Copyright 2019
 * Ubiquitous Knowledge Processing (UKP) Lab
 * Technische Universit√§t Darmstadt
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program. If not, see http://www.gnu.org/licenses/.
 */
package org.dkpro.tc.examples.deeplearning.dl4j.document;

import static org.nd4j.linalg.indexing.NDArrayIndex.all;
import static org.nd4j.linalg.indexing.NDArrayIndex.point;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.NoSuchElementException;

import org.apache.commons.io.FileUtils;
import org.apache.commons.lang3.tuple.Pair;
import org.deeplearning4j.models.embeddings.wordvectors.WordVectors;
import org.deeplearning4j.text.tokenization.tokenizerfactory.TokenizerFactory;
import org.dkpro.tc.core.DeepLearningConstants;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.dataset.DataSet;
import org.nd4j.linalg.dataset.api.DataSetPreProcessor;
import org.nd4j.linalg.dataset.api.iterator.DataSetIterator;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.INDArrayIndex;

//Slightly modified version taken from the official deeplearning4j-example project
@SuppressWarnings("serial")
public class NewsIterator
    implements DataSetIterator
{
    private final WordVectors wordVectors;
    private final int batchSize;
    private final int vectorSize;
    private int maxLength;
    private final String dataDirectory;
    private final List<Pair<String, List<String>>> categoryData = new ArrayList<>();
    private int cursor = 0;
    private int totalNews = 0;
    private int newsPosition = 0;
    private final List<String> labels;
    private int currCategory = 0;

    /**
     * @param dataDirectory
     *            the directory of the news headlines data set
     * @param wordVectors
     *            WordVectors object
     * @param batchSize
     *            Size of each minibatch for training
     * @param truncateLength
     *            If headline length exceed this size, it will be truncated to this size.
     * @param train
     *            If true: return the training data. If false: return the testing data.
     *            <p>
     *            - initialize various class variables - calls populateData function to load news
     *            data in categoryData vector - also populates labels (i.e. category related
     *            inforamtion) in labels class variable
     * @throws Exception
     */
    private NewsIterator(String dataDirectory, WordVectors wordVectors, int batchSize)
        throws Exception
    {
        this.dataDirectory = dataDirectory;
        this.batchSize = batchSize;
        this.vectorSize = wordVectors.getWordVector(wordVectors.vocab().wordAtIndex(0)).length;
        this.wordVectors = wordVectors;
        this.populateData();
        this.labels = new ArrayList<>();
        for (int i = 0; i < this.categoryData.size(); i++) {
            this.labels.add(this.categoryData.get(i).getKey());
        }
    }

    public static Builder builder()
    {
        return new Builder();
    }

    @Override
    public DataSet next(int num)
    {
        if (cursor >= this.totalNews)
            throw new NoSuchElementException();
        try {
            return nextDataSet(num);
        }
        catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

    private DataSet nextDataSet(int num) throws IOException
    {
        List<String> news = new ArrayList<>(num);
        int[] category = new int[num];

        for (int i = 0; i < num && cursor < totalExamples(); i++) {
            if (currCategory < categoryData.size()) {
                news.add(this.categoryData.get(currCategory).getValue().get(newsPosition));
                category[i] = Integer.parseInt(this.categoryData.get(currCategory).getKey());
                currCategory++;
                cursor++;
            }
            else {
                currCategory = 0;
                newsPosition++;
                i--;
            }
        }

        // Second: tokenize news and filter out unknown words
        List<List<String>> allTokens = new ArrayList<>(news.size());
        maxLength = 0;
        for (String s : news) {
            List<String> asList = Arrays.asList(s.replaceAll(" 0", "").split(" "));
            maxLength = maxLength < asList.size() ? asList.size() : maxLength;
            allTokens.add(asList);
        }

        // If longest news exceeds 'truncateLength': only take the first 'truncateLength' words
        // System.out.println("maxLength : " + maxLength);

        // Create data for training
        // Here: we have news.size() examples of varying lengths
        INDArray features = Nd4j.create(news.size(), vectorSize, maxLength);
        INDArray labels = Nd4j.create(news.size(), this.categoryData.size(), maxLength); // Three
                                                                                         // labels:
                                                                                         // Crime,
                                                                                         // Politics,
                                                                                         // Bollywood

        // Because we are dealing with news of different lengths and only one output at the final
        // time step: use padding arrays
        // Mask arrays contain 1 if data is present at that time step for that example, or 0 if data
        // is just padding
        INDArray featuresMask = Nd4j.zeros(news.size(), maxLength);
        INDArray labelsMask = Nd4j.zeros(news.size(), maxLength);

        int[] temp = new int[2];
        for (int i = 0; i < news.size(); i++) {
            List<String> tokens = allTokens.get(i);
            temp[0] = i;
            // Get word vectors for each word in news, and put them in the training data
            for (int j = 0; j < tokens.size() && j < maxLength; j++) {
                String token = tokens.get(j);
                INDArray vector = wordVectors.getWordVectorMatrix(token);
                features.put(new INDArrayIndex[] { point(i), all(), point(j) }, vector);

                temp[1] = j;
                featuresMask.putScalar(temp, 1.0);
            }
            int idx = category[i] - 1; // Index starts at 1
            int lastIdx = Math.min(tokens.size(), maxLength);
            labels.putScalar(new int[] { i, idx, lastIdx - 1 }, 1.0);
            labelsMask.putScalar(new int[] { i, lastIdx - 1 }, 1.0);
        }

        DataSet ds = new DataSet(features, labels, featuresMask, labelsMask);
        return ds;
    }

    /**
     * Used post training to load a review from a file to a features INDArray that can be passed to
     * the network output method
     *
     * @param file
     *            File to load the review from
     * @param maxLength
     *            Maximum length (if review is longer than this: truncate to maxLength). Use
     *            Integer.MAX_VALUE to not nruncate
     * @return Features array
     * @throws IOException
     *             If file cannot be read
     */
    public INDArray loadFeaturesFromFile(File file, int maxLength) throws IOException
    {
        @SuppressWarnings("deprecation")
        String news = FileUtils.readFileToString(file);
        return loadFeaturesFromString(news, maxLength);
    }

    /**
     * Used post training to convert a String to a features INDArray that can be passed to the
     * network output method
     *
     * @param reviewContents
     *            Contents of the review to vectorize
     * @param maxLength
     *            Maximum length (if review is longer than this: truncate to maxLength). Use
     *            Integer.MAX_VALUE to not nruncate
     * @return Features array for the given input String
     */
    public INDArray loadFeaturesFromString(String reviewContents, int maxLength)
    {
        // List<String> tokens =
        // tokenizerFactory.create(reviewContents).getTokens();
        List<String> tokens = new ArrayList<>();
        List<String> tokensFiltered = new ArrayList<>();
        for (String t : tokens) {
            if (wordVectors.hasWord(t))
                tokensFiltered.add(t);
        }
        int outputLength = Math.max(maxLength, tokensFiltered.size());

        INDArray features = Nd4j.create(1, vectorSize, outputLength);

        for (int j = 0; j < tokens.size() && j < maxLength; j++) {
            String token = tokens.get(j);
            INDArray vector = wordVectors.getWordVectorMatrix(token);
            features.put(new INDArrayIndex[] { point(0), all(), point(j) }, vector);
        }

        return features;
    }

    private void populateData() throws Exception
    {

        // FIXME: Implement me a bit more efficiently :)
        List<String> vectors = FileUtils.readLines(
                new File(this.dataDirectory, DeepLearningConstants.FILENAME_INSTANCE_VECTOR),
                "utf-8");
        String outcomes = FileUtils.readLines(
                new File(this.dataDirectory, DeepLearningConstants.FILENAME_OUTCOME_VECTOR),
                "utf-8").get(0);

        // FIXME: We assume that our outcomes have only 1 digit ....
        outcomes = outcomes.replaceAll(" ", "");

        Map<String, List<String>> m = new HashMap<>();

        for (int i = 0; i < vectors.size(); i++) {
            String v = vectors.get(i);
            v = v.substring(1, v.length() - 1);
            String o = outcomes.charAt(i) + "";

            List<String> list = m.get(o);
            if (list == null) {
                list = new ArrayList<>();
            }
            list.add(v);
            m.put(o, list);

            this.totalNews++;
        }

        for(Entry<String, List<String>> k: m.entrySet()) {
            Pair<String, List<String>> tempPair = Pair.of(k.getKey(), k.getValue());
            this.categoryData.add(tempPair);
        }
    }

    @Override
    public int totalExamples()
    {
        return this.totalNews;
    }

    @Override
    public int inputColumns()
    {
        return vectorSize;
    }

    @Override
    public int totalOutcomes()
    {
        return this.categoryData.size();
    }

    @Override
    public void reset()
    {
        cursor = 0;
        newsPosition = 0;
        currCategory = 0;
    }

    public boolean resetSupported()
    {
        return true;
    }

    @Override
    public boolean asyncSupported()
    {
        return true;
    }

    @Override
    public int batch()
    {
        return batchSize;
    }

    @Override
    public int cursor()
    {
        return cursor;
    }

    @Override
    public int numExamples()
    {
        return totalExamples();
    }

    @Override
    public void setPreProcessor(DataSetPreProcessor preProcessor)
    {
        throw new UnsupportedOperationException();
    }

    @Override
    public List<String> getLabels()
    {
        return this.labels;
    }

    @Override
    public boolean hasNext()
    {
        return cursor < numExamples();
    }

    @Override
    public DataSet next()
    {
        return next(batchSize);
    }

    @Override
    public void remove()
    {

    }

    @Override
    public DataSetPreProcessor getPreProcessor()
    {
        throw new UnsupportedOperationException("Not implemented");
    }

    public int getMaxLength()
    {
        return this.maxLength;
    }

    public static class Builder
    {
        private String dataDirectory;
        private WordVectors wordVectors;
        private int batchSize;

        Builder()
        {
        }

        public NewsIterator.Builder dataDirectory(String dataDirectory)
        {
            this.dataDirectory = dataDirectory;
            return this;
        }

        public NewsIterator.Builder wordVectors(WordVectors wordVectors)
        {
            this.wordVectors = wordVectors;
            return this;
        }

        public NewsIterator.Builder batchSize(int batchSize)
        {
            this.batchSize = batchSize;
            return this;
        }

        public NewsIterator.Builder tokenizerFactory(TokenizerFactory tokenizerFactory)
        {
            return this;
        }

        public NewsIterator build() throws Exception
        {
            return new NewsIterator(dataDirectory, wordVectors, batchSize);
        }

        public String toString()
        {
            return "org.deeplearning4j.examples.recurrent.ProcessNews.NewsIterator.Builder(dataDirectory="
                    + this.dataDirectory + ", wordVectors=" + this.wordVectors + ", batchSize="
                    + this.batchSize + ")";
        }
    }
}
